\documentclass{sig-alternate-ipsn13}

\usepackage{cite}

\begin{document}

\title{Reactive Scheduling of Computational \\ Resources in Control Systems}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
    \alignauthor Gera Weiss\\
       \affaddr{Ben-Gurion University}\\
       \affaddr{Beer-Sheva, Israel}\\
       \email{geraw@cs.bgu.ac.il}
% 2th. author
    \alignauthor Hodai Goldman\\
        \affaddr{Ben-Gurion University}\\
        \affaddr{Beer-Sheva, Israel}\\
        \email{hodaig@cs.bgu.ac.il}
}

\maketitle
\begin{abstract}

\end{abstract}

\section{Introduction}
Cyber-physical systems (CPS) technologies of integrating software and control are at the heart of many critical applications (c.f.~\cite{lee2008cyber}). 
These technologies aim at handling issues that emerge when the integration of software and hardware brakes the traditional abstraction layers: when researchers and practitioners are required to consider a unified view that includes both software and hardware. An example of such an issue is the challenge of dynamic assignment of computational resources to software based controllers discussed in, e.g.,~\cite{arzen2000introduction,tabuada2007event,weiss2007automata}. While the computation burden required by the control loops can be ignored in many situations, this is not always the case. A main motivating example studied in this paper is vision based control, where computer vision algorithms acquire state information to be used in a feedback loop (c.f.~\cite{das2002vision,shakernia1999landing,Efraim2017}). Unlike conventional sensors such as accelerometers, gyros, compasses, etc., a visual sensor requires significant processing of the acquired image to prepare the state information for feedback. Since typical cyber-physical application, such as robot control, consist of many control loops, responsible for different aspects of the system, that run simultaneously and share the same computational resources, the computer vision algorithms cannot always be invoked in full power. Alternatively, we propose in this paper a mechanism to dynamically trade CPU consumption vs. measurement accuracy so that data acquisition algorithms run in full power only when the control loop requires accurate data. 

A main challenge in forming mechanisms for the integration of software and control lies in the design of efficient interfaces for integrating the engineering diciplines involved (c.f.~\cite{weiss2007automata}). Components with clearly specified APIs, such as Java library classes, allow designers to build
complex systems effectively in many application domains.  The key to such modular development is
that an individual component can be designed, analyzed, and tested without the knowledge of other
components or the underlying computing platform. When the system contains components with
real-time requirements, the notion of an interface must include the requirements regarding
resources, and existing programming languages provide little support for this.  Consequently,
current development of real-time embedded software requires significant low-level manual effort for
debugging and component assembly (cf.  \cite{Lee00,IEEE03,HS06}).  This has motivated many
researchers to develop compositional approaches and interface notions for real-time scheduling (cf.
\cite{RS01,dH01,MF01,CAHS03,SL08,SLBS04,TWS06,DBLP:conf/lctrts/AuerbachBIKRRT07}).


In this paper we present an approach, a proof-of-concept tool, and a case-study in scheduling computations in embedded control systems. Our approach is based on the automata based scheduling approach, suggested in~\cite{WA07,RTComposer,AW08}, where automata are proposed as interfaces that allow the dynamicity and efficiency of desktop operating systems with the predictability of real-time operating systems. The approach allows for components to specify the CPU resources that they need in a way that gives an application agnostic scheduler the freedom to choose schedules at run-time such that the needs of all the components are met, even of components that were added only at run-time.  The main contributions of this paper relative to the earlier work in this direction is:
(1) We propose an extension of the automata based scheduling frame work that allows to direct the schedule based on the state of the controllers; (2) We propose a technique, based on the theory of Kalman Filters, for designing reactively scheduled controllers; (3) We report on our experience with improving the performance of real-time a vision-based control system (a quadrotor that stabilizes itself in front of a window).

\section{Architecture: Automata Based Reactive Scheduler}

Sections go here.

\section{Simulations}
\label{sec:simulation}
%TODO - show some matlab simulations of hybrid systems.

\section{Proposed Methodology}
\begin{enumerate}
    
    \item \textbf{Controller design:} Based on the separation principle, we propose to design the controller to achieve the control objectives assuming a perfect observation. In practice, this may not be feasible because controller designs such as PID require a system to experiment with. In this case, as demonstrated in the case-study below (see Section~\ref{sec:caseStady}), we propose to work with one of the observation modes. If the system is close to linear, this should result with a near optimal design.
    
    \item \textbf{Observers design:} Specification of sensor modes and observers design
    
    \item \textbf{Performance analysis:} Now, we can perform some experiments with the different observer modes and analyze transient behaviors. Specifically, as shown in the case study~\ref{sec:caseStady_analysis}, we can measure how long it takes for the error to accumulate after switching to a lesser observation mode and formulate how this error affects the control objectives.

    \item \textbf{Scheduling automata design:} Base on the analysis we can specify the resource scheduling requirements in the form of \textit{specification automaton}. The goal is to design flexible specification that allow dynamic scheduling in order to adapt the environment and the system state, this will improve the system efficient.

\end{enumerate}


%\section{Application to Autonomous Quad-rotor Flying In-Door}
\section{Application: Stabilizing a quadrotor in front of a window}
\label{sec:caseStady}
% overview of why we use vision example

%TODO - Window test case problem
The case study we used to test our concept is the development of a controller that stabilizes a quadrotor~\cite{?} in front of a window.
We implement an autonomous controller for that task and evaluated its performance.

The part of the controller that we focused on is the vision based position controller. Specifically, the main controller, that we will describe below, uses a standard low-level angular controller and a simple image processing algorithm that identifies the position of the corners of the window in the image plane\footnote{In the experiment, to simplify the image processing algorithm, we marked the corners of the window with led lights.}. Its goal is to regulate the position of the quadrotor by tilting it. Note that rotations of the quadrotor generate a more-or-less proportional accelerations in a corresponding direction. A main challenge for this controller is that the computer vision algorithm takes significant time to compute relative to the fast control loop. We can decrease computation time by lowering the resolution, but this also increase the measurement noise. We will demonstrate how adaptive scheduling of the resolution can serve for balancing resource consumption vs. control performance.

\subsection{Observer Design}
We first implemented an observer based on the work of Hanoch~\cite{Hanoch}. The observer gets the positions of the widow corners, enumerated clockwise starting from the top left corner noted by $p_1,p_2,p_3,p_4$, and extract 4 quantities based on the shape and location of the window corners in the image plane: $S_x$ , $s_y$ , $V_d$ and $sz$.
\textbf{center of mass:} $S_x$ and $S_y$ represent the window ``center of mass'' in the image plane along the image $x$ and $y$ axes, respectively. $S_x$ and $S_y$ are normalized to the range of $[-1,1]$.
$S_x$ is used to measure the roll angle of the window center (for stabilize the roll axis), and $S_y$ is used to measure the altitude of the drone (for stabilizing the throttle).
\textbf{window size:} $sz$ is the sum of the vertical edges of the window, $sz$ is used to measure the distance of the drone from the window and then to regulate the roll angle~\footnote{we use fixed size window and convert $sz$ to distance (in meter in our case) based on this window, in the general case the distance is relative to the window size}.
\textbf{Vertical difference:} $V_d = \frac{y_1-y_4-(y_2-y_3)}{y_1-y_4+(y_2-y_3)}$, were $y_i$ is the vertical position of $p_i$ in the range of $[-1,1]$ (0 is the center of the image) first is used to measure the angular position of the drone in relation to the window ($\theta$), then $\theta$ and $sz$ are used to calculate the horizontal position parallel to the window surface ($x$ position) of the drone (see Figure~\ref{fig:cordinates}). 

%TODO - http://robottini.altervista.org/kalman-filter-vs-complementary-filter
%TODO - maybe we can cite this: http://proyectos.ciii.frc.utn.edu.ar/cuadricoptero/export/262a5075a7aa6575102f84fcdc2ee39ecab3c530/documentacion/articulo_case_filtro_comp/referencias/A%20comparison%20of%20complementary%20and%20kalman%20filtering.pdf
After measuring the relative position and yaw attitude as usual we add estimator (filter) to get the best state estimation. 
As shown in the simulation at Section~\ref{sec:simulation} we should use Kalman filter estimator, in our case we have non-linear system... %TODO - continue
For simplicity, inspiring from linear Kalman filter~\cite{??? kalman}, we use two steps estimator that \textit{predict} the current state evolved from the previous state using a linearized model of the system ($\hat{x_{k|k-1}}$) and then \textit{update} the prediction with current state measurement from image explained before, noted by $z_k$.
This is a simplified kalman filter, a known technique as show in~\cite{??? kalman vs CF}, where The result estimation (noted by $\hat{x}_{k|k}$) is a complementary filter of the prediction ($\hat{x}_{k|k-1}$) and the measurement ($z_k$):
$ \hat{x_{k|k}} = K \hat{x}_{k|k-1} + (1-K) z_k $.
%TODO - re-write the above paragraph










%TODO - continue here
As describe in Section~\ref{sec:???}, the image processing have few different operation modes, every mode use different image resolution, better image resolution results in more accurate measurement but consume more processing time, changing operation modes allows us to control the trade off between mesurment quality and processing time as shown in Table~\ref{tab:tradeoff ???}.
The $K$ gain is tuned different for each mode (resolution) for achieving the best estimation in each mode.
For simplicity, we examine only two modes, \textit{High quality} with resolution of 960p and \textit{Low quality} with resolution of 240p.

\subsection{Controller Design}
We used a simple Proportional Integral Derivative (PID~\cite{aastrom2006advanced}) controller that we tuned by experimenting with the quadrotor in our lab. Based on the separation principle~\cite{?}, we tuned the parameters of our controller only with the highest resolution observer and used them for all the other observers without any modification.

% in terms of the horizontal linear axis parallel to the window surface, as this is the most critical axis in this task, this axis noted by $x$ see Figure~\ref{fig:window axis ???}.
%We implement standard PID~\cite{aastrom2006advanced} controller that regulates the position and attitude of the drone relative to the window.
%We used inertial sensors (Gyroscope and Accelerometer) for attitude control relative to earth, and we use camera~\cite{??? picam} and simple image processing for position and attitude relatively to the window~\ref{sec:visionAlgo ???}.


\subsection{Analysis and Specification Automata}

The objective of the system is to maintain stable hovering in front of the window. Hence, the performances of the system is measured by the amount of deviation from the center line of the window in the critical axis $x$ (see Figure~\ref{fig:window axis ???}). Our goal is to achieve maximum performance with minimal amount of processing (CPU percentage). The main consumer of computation resources in this system is the heavy observation tasks.

Analyzing the test results shown in Table~\ref{tab:res ???} we see that High quality observation mode provide 0.4 meter error tolerance meaning we can fly this mode in 1.2 meter wide corridor\footnote{Experiments was done with 0.4 meter wide quad-rotor}, with the cost of 30\% CPU usage. 
With adaptive scheduling specification we lower the CPU usage without significant worsening of High quality performances.

%TODO - error ($y_{k|k}$) derived schedulers
From the Low quality experiment graph we can see that measurement post-fit residual ($\tilde{y}_{k|k}$) is accumulate proportional to the deviation from the center of $x$ axis, and we can use $\tilde{y}_{k|k}$ vales to predict 


TODO - position ($x_{k|k}$)  derived schedulers

TODO - complex / agregated schedulers

TODO - gyro derived??


\subsection{Results}
TODO - graphs and tables

\section{Experiment setup}
The drone is controlled by a Raspberry pi with the navio~\cite{??? raspberry, navio} that runs a modified ArduPilot~\cite{??? APM} software.

%TODO - natnet
%TODO - vision search algorithem??

\section{Conclusions}
Conclusion goes here.


%ACKNOWLEDGMENTS are optional
\section*{Acknowledgments}
Acknowledgement goes here.


%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{reactive_scheduler}  % reactive_scheduler.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
\appendix
%Appendix A

Appendix goes here.

% That's all folks!
\end{document}
